#!/usr/bin/env python3
"""
–û—Ü–µ–Ω–∫–∞ texture-aware –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
10 —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ √ó 10 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π = 100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–ú–µ—Ç—Ä–∏–∫–∏: accuracy, recall, top-5 accuracy, confidence
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
import keras
import tensorflow_datasets as tfds
import numpy as np
from sklearn.metrics import recall_score, precision_score, f1_score
import json
import random

print("=" * 80)
print("üî¨ –û–¶–ï–ù–ö–ê TEXTURE-AWARE –ú–û–î–ï–õ–ò –ù–ê –°–õ–£–ß–ê–ô–ù–û–ô –í–´–ë–û–†–ö–ï")
print("=" * 80)
print()

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model = keras.models.load_model('models/texture_improved_perfect.keras')
print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
print()

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤
with open('class_names.json', 'r') as f:
    class_names = json.load(f)

print(f"üìã –í—Å–µ–≥–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
print()

# –ó–∞–≥—Ä—É–∑–∫–∞ validation –¥–∞—Ç–∞—Å–µ—Ç–∞
print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ Food-101 validation –¥–∞—Ç–∞—Å–µ—Ç–∞...")
ds_val = tfds.load(
    'food101',
    split='validation',
    as_supervised=True,
    shuffle_files=False
)
print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
print()

# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
def preprocess(image, label):
    image = tf.image.resize(image, [224, 224])
    image = keras.applications.resnet50.preprocess_input(image)
    return image, label

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
all_data = list(ds_val.as_numpy_iterator())
print(f"‚úÖ –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(all_data)}")
print()

# –í—ã–±–∏—Ä–∞–µ–º 10 —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
selected_classes = random.sample(range(101), 10)
selected_classes_names = [class_names[i] for i in selected_classes]

print("üé≤ –°–ª—É—á–∞–π–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã:")
for i, class_idx in enumerate(selected_classes, 1):
    print(f"   {i}. {class_names[class_idx].replace('_', ' ').title()} (–∫–ª–∞—Å—Å {class_idx})")
print()

# –°–æ–±–∏—Ä–∞–µ–º –ø–æ 10 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
print("üì∏ –°–±–æ—Ä –ø–æ 10 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞...")
selected_data = []
class_counts = {cls: 0 for cls in selected_classes}

for image, label in all_data:
    if label in selected_classes and class_counts[label] < 10:
        selected_data.append((image, label))
        class_counts[label] += 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–±—Ä–∞–ª–∏ –ª–∏ –º—ã –≤—Å—ë
        if all(count == 10 for count in class_counts.values()):
            break

print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(selected_data)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
print()

# –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
def data_generator():
    for image, label in selected_data:
        yield image, label

ds_selected = tf.data.Dataset.from_generator(
    data_generator,
    output_signature=(
        tf.TensorSpec(shape=(None, None, 3), dtype=tf.uint8),
        tf.TensorSpec(shape=(), dtype=tf.int64)
    )
)

# –ü—Ä–∏–º–µ–Ω—è–µ–º preprocessing
ds_selected_prep = (
    ds_selected
    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(10)
    .prefetch(tf.data.AUTOTUNE)
)

print("üß™ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—ã–±–æ—Ä–∫–µ (100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)...")
print()

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
all_predictions = []
all_top5_predictions = []
all_confidences = []
all_labels = []

for images, labels in ds_selected_prep:
    predictions = model.predict(images, verbose=0)

    # Top-1 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predicted_classes = np.argmax(predictions, axis=1)
    all_predictions.extend(predicted_classes)

    # Top-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    top5_indices = np.argsort(predictions, axis=1)[:, -5:]
    all_top5_predictions.extend(top5_indices)

    # Confidence (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)
    confidences = np.max(predictions, axis=1)
    all_confidences.extend(confidences)

    # –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    all_labels.extend(labels.numpy())

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy arrays
all_predictions = np.array(all_predictions)
all_top5_predictions = np.array(all_top5_predictions)
all_confidences = np.array(all_confidences)
all_labels = np.array(all_labels)

# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
accuracy = np.mean(all_predictions == all_labels)
average_confidence = np.mean(all_confidences)

# Recall (macro - —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–ª–∞—Å—Å–∞–º)
recall_macro = recall_score(all_labels, all_predictions, labels=selected_classes, average='macro', zero_division=0)

# Recall –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
recall_per_class = recall_score(all_labels, all_predictions, labels=selected_classes, average=None, zero_division=0)

# Top-5 Accuracy
top5_correct = 0
for i, label in enumerate(all_labels):
    if label in all_top5_predictions[i]:
        top5_correct += 1
top5_accuracy = top5_correct / len(all_labels)

print("=" * 80)
print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò")
print("=" * 80)
print()
print(f"üéØ Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"üéØ Top-5 Accuracy:     {top5_accuracy:.4f} ({top5_accuracy*100:.2f}%)")
print(f"üìà Recall (macro):     {recall_macro:.4f} ({recall_macro*100:.2f}%)")
print(f"üí™ Avg Confidence:     {average_confidence:.4f} ({average_confidence*100:.2f}%)")
print()

# –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print("üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:        {len(all_labels)}")
print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (Top-1):        {np.sum(all_predictions == all_labels)}")
print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ (Top-5):        {top5_correct}")
print(f"   –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ:              {np.sum(all_predictions != all_labels)}")
print(f"   –°—Ä–µ–¥–Ω—è—è confidence:       {average_confidence*100:.2f}%")
print(f"   –ú–∏–Ω. confidence:          {np.min(all_confidences)*100:.2f}%")
print(f"   –ú–∞–∫—Å. confidence:         {np.max(all_confidences)*100:.2f}%")
print()

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
print("üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print()
for i, class_idx in enumerate(selected_classes):
    class_mask = all_labels == class_idx
    class_predictions = all_predictions[class_mask]
    class_labels = all_labels[class_mask]
    class_confidences = all_confidences[class_mask]

    class_accuracy = np.mean(class_predictions == class_labels)
    class_confidence = np.mean(class_confidences)
    class_recall = recall_per_class[i]

    # Top-5 –¥–ª—è –∫–ª–∞—Å—Å–∞
    class_top5 = all_top5_predictions[class_mask]
    class_top5_correct = np.sum([class_idx in top5 for top5 in class_top5])
    class_top5_accuracy = class_top5_correct / len(class_labels)

    class_name = class_names[class_idx].replace('_', ' ').title()
    print(f"   {class_name:25} | Acc: {class_accuracy*100:5.1f}% | Recall: {class_recall*100:5.1f}% | Top-5: {class_top5_accuracy*100:5.1f}% | Conf: {class_confidence*100:5.1f}%")

print()

# –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
print("‚ùå –ü—Ä–∏–º–µ—Ä—ã –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
print()
errors_shown = 0
for i, (label, pred, conf) in enumerate(zip(all_labels, all_predictions, all_confidences)):
    if label != pred and errors_shown < 5:
        true_name = class_names[label].replace('_', ' ').title()
        pred_name = class_names[pred].replace('_', ' ').title()
        print(f"   –ü—Ä–∏–º–µ—Ä {i+1}: –ò—Å—Ç–∏–Ω–Ω—ã–π: {true_name} ‚Üí –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω: {pred_name} (Conf: {conf*100:.1f}%)")
        errors_shown += 1

if errors_shown == 0:
    print("   –ù–µ—Ç –æ—à–∏–±–æ–∫! –í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Ä–Ω—ã!")

print()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
with open('texture_model_random_sample_results.txt', 'w', encoding='utf-8') as f:
    f.write("TEXTURE-AWARE MODEL RANDOM SAMPLE EVALUATION\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"Sample Size: 100 images (10 classes √ó 10 images)\n")
    f.write(f"Random Seed: 42\n\n")

    f.write("SELECTED CLASSES:\n")
    for i, class_idx in enumerate(selected_classes, 1):
        f.write(f"{i}. {class_names[class_idx]}\n")
    f.write("\n")

    f.write("OVERALL METRICS:\n")
    f.write(f"Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\n")
    f.write(f"Top-5 Accuracy:     {top5_accuracy:.4f} ({top5_accuracy*100:.2f}%)\n")
    f.write(f"Recall (macro):     {recall_macro:.4f} ({recall_macro*100:.2f}%)\n")
    f.write(f"Avg Confidence:     {average_confidence:.4f} ({average_confidence*100:.2f}%)\n")
    f.write("\n")

    f.write("PER-CLASS PERFORMANCE:\n")
    for i, class_idx in enumerate(selected_classes):
        class_mask = all_labels == class_idx
        class_predictions = all_predictions[class_mask]
        class_labels = all_labels[class_mask]
        class_confidences = all_confidences[class_mask]

        class_accuracy = np.mean(class_predictions == class_labels)
        class_confidence = np.mean(class_confidences)
        class_recall = recall_per_class[i]

        class_top5 = all_top5_predictions[class_mask]
        class_top5_correct = np.sum([class_idx in top5 for top5 in class_top5])
        class_top5_accuracy = class_top5_correct / len(class_labels)

        f.write(f"{class_names[class_idx]:30} | Acc: {class_accuracy*100:5.1f}% | Recall: {class_recall*100:5.1f}% | Top-5: {class_top5_accuracy*100:5.1f}% | Conf: {class_confidence*100:5.1f}%\n")

print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: texture_model_random_sample_results.txt")
print()
print("=" * 80)
