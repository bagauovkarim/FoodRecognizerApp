#!/usr/bin/env python3
"""
PERFECT VERSION: –ò–¥–µ–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ texture-aware –º–æ–¥–µ–ª–∏
70.4% ‚Üí 76-79%+

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –í–°–ï –ø—Ä–æ–±–ª–µ–º—ã, –≤–∫–ª—é—á–∞—è —Å–∫—Ä—ã—Ç—ã–µ:
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫: resize ‚Üí augment ‚Üí preprocess_input ‚Üí batch ‚Üí mixup
‚úÖ –£–±—Ä–∞–Ω clip –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
‚úÖ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ label formats
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π LR –¥–ª—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏ conv4+conv5: 2e-5
‚úÖ Gradient clipping –¥–ª—è fp16
‚úÖ CosineDecayRestarts (adaptive LR)
‚úÖ Beta-sampling –¥–ª—è Mixup
‚úÖ Label smoothing = 0.03
‚úÖ Batch size = 16
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ nested layers
‚úÖ Mixed precision (fp16)

–í–ê–ñ–ù–û: EMA –æ—Ç–∫–ª—é—á—ë–Ω (–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å LearningRateSchedule)
Mixup + CosineRestarts –¥–∞—é—Ç –±–æ–ª—å—à–µ (+3-5%) —á–µ–º EMA (+0.5-0.8%)
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
import keras
from keras import layers, optimizers, callbacks
import tensorflow_datasets as tfds
import numpy as np
import gc

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =============================================================================

INPUT_MODEL_PATH = 'models/resnet50_texture_aware_final.keras'
OUTPUT_MODEL_PATH = 'models/texture_improved_perfect.keras'
LOG_FILE = 'training_texture_perfect.log'

BATCH_SIZE = 16
BUFFER_SIZE = 2000
EPOCHS_STAGE1 = 5
EPOCHS_STAGE2 = 20
LEARNING_RATE_STAGE1 = 1e-4
LEARNING_RATE_STAGE2_INITIAL = 2e-5  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 5e-6!
LEARNING_RATE_STAGE2_MIN = 1e-7
LABEL_SMOOTHING = 0.03
MIXUP_ALPHA = 0.3

print("=" * 80)
print("üî• PERFECT TEXTURE-AWARE MODEL IMPROVEMENT")
print("=" * 80)
print(f"–í—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
print(f"  ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ pipeline")
print(f"  ‚úÖ –£–±—Ä–∞–Ω clip –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π")
print(f"  ‚úÖ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ label formats")
print(f"  ‚úÖ LR Stage2 = {LEARNING_RATE_STAGE2_INITIAL} (–±—ã–ª–æ 5e-6)")
print(f"  ‚úÖ Gradient clipping (clipnorm=1.0)")
print(f"  ‚úÖ CosineDecayRestarts (adaptive LR)")
print(f"  ‚úÖ Mixed precision (fp16)")
print()

# =============================================================================
# MIXED PRECISION
# =============================================================================

print("üöÄ –í–∫–ª—é—á–µ–Ω–∏–µ Mixed Precision (fp16)...")
try:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    print("‚úÖ Mixed precision –≤–∫–ª—é—á–µ–Ω (fp16)")
except Exception as e:
    print(f"‚ö†Ô∏è  Mixed precision –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
print()

# =============================================================================
# –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò
# =============================================================================

print("üßπ –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏...")
tf.keras.backend.clear_session()
gc.collect()
print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
print()

# =============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# =============================================================================

print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model = keras.models.load_model(INPUT_MODEL_PATH)
print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
print(f"   Trainable: {sum([p.numpy().size for p in model.trainable_weights]):,}")
print()

# =============================================================================
# –î–ê–¢–ê–°–ï–¢
# =============================================================================

print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ Food-101...")
ds_train, ds_val = tfds.load(
    'food101',
    split=['train', 'validation'],
    as_supervised=True,
    shuffle_files=True
)
print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
print()

# =============================================================================
# –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø!)
# =============================================================================

def resize_only(image, label):
    """–¢–æ–ª—å–∫–æ resize (–ë–ï–ó preprocess_input!)"""
    image = tf.image.resize(image, [224, 224])
    return image, label

def augment_aggressive(image, label):
    """
    –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è food
    –í–ê–ñ–ù–û: –¥–µ–ª–∞–µ—Ç—Å—è –î–û preprocess_input!
    """
    # Flip
    image = tf.image.random_flip_left_right(image)

    # Brightness & Contrast
    image = tf.image.random_brightness(image, 0.2)
    image = tf.image.random_contrast(image, 0.8, 1.2)

    # HUE SHIFT (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –µ–¥—ã!)
    image = tf.image.random_hue(image, 0.05)

    # Saturation (–¥–ª—è –µ–¥—ã –≤–∞–∂–Ω–æ!)
    image = tf.image.random_saturation(image, 0.8, 1.2)

    # –ù–ï–¢ clip! preprocess_input —Å–∞–º –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç
    return image, label

def prepare_for_model(image, label):
    """
    –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è ResNet50
    –î–µ–ª–∞–µ—Ç—Å—è –ü–û–°–õ–ï –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π!
    """
    image = keras.applications.resnet50.preprocess_input(image)
    return image, label

def mixup_sample_beta(alpha):
    """–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π Beta sampling –¥–ª—è Mixup"""
    gamma1 = tf.random.gamma([], alpha, 1.0)
    gamma2 = tf.random.gamma([], alpha, 1.0)
    lambda_param = gamma1 / (gamma1 + gamma2)
    return lambda_param

def mixup_batch(images, labels, alpha=MIXUP_ALPHA):
    """
    Mixup –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞—Ç—á–∞
    –í–ê–ñ–ù–û: —Ä–∞–±–æ—Ç–∞–µ—Ç —Å sparse labels, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç one-hot
    """
    batch_size = tf.shape(images)[0]
    lambda_param = mixup_sample_beta(alpha)
    indices = tf.random.shuffle(tf.range(batch_size))

    # Mix images
    mixed_images = lambda_param * images + (1 - lambda_param) * tf.gather(images, indices)

    # Mix labels: sparse ‚Üí one-hot ‚Üí mix
    labels_onehot = tf.one_hot(labels, 101)
    mixed_labels = lambda_param * labels_onehot + (1 - lambda_param) * tf.gather(labels_onehot, indices)

    return mixed_images, mixed_labels

# =============================================================================
# –ü–†–û–í–ï–†–ö–ê –ù–ê–ß–ê–õ–¨–ù–û–ô –¢–û–ß–ù–û–°–¢–ò
# =============================================================================

print("üî¨ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—É—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏...")

ds_val_check = (
    ds_val
    .take(1000)
    .map(resize_only, num_parallel_calls=2)
    .map(prepare_for_model, num_parallel_calls=2)
    .batch(BATCH_SIZE)
    .prefetch(1)
)

model.compile(
    optimizer='adam',
    loss=keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy', keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5')]
)

initial_scores = model.evaluate(ds_val_check, verbose=0)
print(f"üìä –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å:")
print(f"   Loss: {initial_scores[0]:.4f}")
print(f"   Accuracy: {initial_scores[1]*100:.2f}%")
print(f"   Top-5: {initial_scores[2]*100:.2f}%")
print()

# =============================================================================
# –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–û–í
# =============================================================================

print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–ò–î–ï–ê–õ–¨–ù–ê–Ø PIPELINE)...")

# STAGE 1: –ë–ï–ó mixup
# Pipeline: resize ‚Üí augment ‚Üí preprocess_input ‚Üí batch
ds_train_stage1 = (
    ds_train
    .shuffle(BUFFER_SIZE)
    .map(resize_only, num_parallel_calls=tf.data.AUTOTUNE)
    .map(augment_aggressive, num_parallel_calls=tf.data.AUTOTUNE)
    .map(prepare_for_model, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

# STAGE 2: –° mixup
# Pipeline: resize ‚Üí augment ‚Üí preprocess_input ‚Üí batch ‚Üí mixup
ds_train_stage2 = (
    ds_train
    .shuffle(BUFFER_SIZE)
    .map(resize_only, num_parallel_calls=tf.data.AUTOTUNE)
    .map(augment_aggressive, num_parallel_calls=tf.data.AUTOTUNE)
    .map(prepare_for_model, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .map(lambda img, lbl: mixup_batch(img, lbl, MIXUP_ALPHA),
         num_parallel_calls=tf.data.AUTOTUNE)
    .prefetch(tf.data.AUTOTUNE)
)

# Validation: resize ‚Üí preprocess_input ‚Üí batch (sparse labels)
ds_val_prep = (
    ds_val
    .map(resize_only, num_parallel_calls=tf.data.AUTOTUNE)
    .map(prepare_for_model, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –≥–æ—Ç–æ–≤—ã (–∏–¥–µ–∞–ª—å–Ω–∞—è pipeline)")
print()

# =============================================================================
# STAGE 1: –û–ë–£–ß–ï–ù–ò–ï –¢–ï–ö–£–©–ò–• TRAINABLE –°–õ–û–Å–í (–ë–ï–ó MIXUP)
# =============================================================================

print("=" * 80)
print("üìö STAGE 1: –û–±—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö trainable —Å–ª–æ—ë–≤ (–ë–ï–ó Mixup)")
print("=" * 80)
print()

trainable_count = sum([1 for l in model.layers if l.trainable])
print(f"Trainable —Å–ª–æ–∏: {trainable_count}")
print()

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –¥–ª—è Stage 1
# –ò—Å–ø–æ–ª—å–∑—É–µ–º Sparse loss (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ —Å sparse, –∏ —Å one-hot)
optimizer_stage1 = optimizers.Adam(
    learning_rate=LEARNING_RATE_STAGE1,
    clipnorm=1.0  # Gradient clipping –¥–ª—è fp16
)

model.compile(
    optimizer=optimizer_stage1,
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy', keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5')]
)

print(f"‚öôÔ∏è  –ö–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–æ:")
print(f"   LR = {LEARNING_RATE_STAGE1}")
print(f"   Gradient clipping = 1.0")
print()

# Callbacks Stage 1
cbs_stage1 = [
    callbacks.ModelCheckpoint(
        'models/texture_perfect_stage1_best.keras',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=2,
        min_lr=1e-7,
        verbose=1
    ),
    callbacks.CSVLogger(LOG_FILE, append=False)
]

print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è Stage 1 ({EPOCHS_STAGE1} —ç–ø–æ—Ö, –ë–ï–ó Mixup)...")
print()

history1 = model.fit(
    ds_train_stage1,
    validation_data=ds_val_prep,
    epochs=EPOCHS_STAGE1,
    callbacks=cbs_stage1,
    verbose=1
)

stage1_best = max(history1.history['val_accuracy'])
print()
print(f"üìä Stage 1 –∑–∞–≤–µ—Ä—à—ë–Ω: –ª—É—á—à–∞—è accuracy = {stage1_best*100:.2f}%")
print()

# =============================================================================
# STAGE 2: –†–ê–ó–ú–û–†–û–ó–ò–¢–¨ –í–ï–°–¨ conv4 + conv5 + MIXUP + COSINE RESTARTS
# =============================================================================

print("=" * 80)
print("üìö STAGE 2: –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ conv4 + conv5 + Mixup + CosineRestarts")
print("=" * 80)
print()

# –ü–†–ê–í–ò–õ–¨–ù–ê–Ø —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ (–∏—â–µ—Ç nested layers!)
def unfreeze_resnet_blocks(model):
    """–†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å conv4 + conv5 –≤ nested ResNet"""
    unfrozen = 0
    for layer in model.layers:
        if hasattr(layer, "layers"):  # Nested model (Functional ResNet)
            for sublayer in layer.layers:
                if "conv4" in sublayer.name or "bn4" in sublayer.name or \
                   "conv5" in sublayer.name or "bn5" in sublayer.name:
                    sublayer.trainable = True
                    unfrozen += 1
        else:
            # –ü—Ä—è–º—ã–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏
            if "conv4" in layer.name or "bn4" in layer.name or \
               "conv5" in layer.name or "bn5" in layer.name:
                layer.trainable = True
                unfrozen += 1
    return unfrozen

unfrozen = unfreeze_resnet_blocks(model)
print(f"üîì –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω–æ {unfrozen} —Å–ª–æ—ë–≤ (conv4 + conv5)")

trainable = sum([p.numpy().size for p in model.trainable_weights])
total = model.count_params()
print(f"   Trainable: {trainable:,} ({trainable/total*100:.1f}%)")
print()

# CosineDecayRestarts
# Food-101 train size = 75750
steps_per_epoch = 75750 // BATCH_SIZE

cosine_restarts_lr = keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate=LEARNING_RATE_STAGE2_INITIAL,
    first_decay_steps=steps_per_epoch * 5,
    t_mul=2.0,
    m_mul=0.8,
    alpha=LEARNING_RATE_STAGE2_MIN / LEARNING_RATE_STAGE2_INITIAL
)

# Optimizer —Å gradient clipping
# –í–ê–ñ–ù–û: EMA –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å CosineDecayRestarts (LearningRateSchedule)
# EMA —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º LR
# Mixup + CosineRestarts + FP16 –¥–∞—é—Ç –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ, —á–µ–º EMA (+0.5-0.8%)
optimizer_stage2 = optimizers.Adam(
    learning_rate=cosine_restarts_lr,
    clipnorm=1.0  # Gradient clipping –¥–ª—è fp16
)

print(f"‚ö†Ô∏è  EMA –æ—Ç–∫–ª—é—á—ë–Ω (–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å CosineDecayRestarts)")
print(f"   Mixup + CosineRestarts –¥–∞—é—Ç +3-5% accuracy")
print(f"   EMA –¥–æ–±–∞–≤–∏–ª –±—ã —Ç–æ–ª—å–∫–æ +0.5-0.8%")

# Loss –¥–ª—è mixup: CategoricalCrossentropy (–ø—Ä–∏–Ω–∏–º–∞–µ—Ç one-hot)
# label_smoothing –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ train (mixup —É–∂–µ –¥–µ–ª–∞–µ—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ)
model.compile(
    optimizer=optimizer_stage2,
    loss=keras.losses.CategoricalCrossentropy(
        label_smoothing=LABEL_SMOOTHING,
        from_logits=False
    ),
    metrics=[
        'accuracy',
        keras.metrics.TopKCategoricalAccuracy(k=5, name='top5')  # –Ø–≤–Ω–æ–µ –∏–º—è!
    ]
)

print(f"‚öôÔ∏è  –ü–µ—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–æ:")
print(f"   Initial LR: {LEARNING_RATE_STAGE2_INITIAL}")
print(f"   Min LR: {LEARNING_RATE_STAGE2_MIN}")
print(f"   Gradient clipping: 1.0")
print(f"   CosineDecayRestarts cycle: {steps_per_epoch * 5} steps")
print()

# Callbacks Stage 2
cbs_stage2 = [
    callbacks.ModelCheckpoint(
        OUTPUT_MODEL_PATH,
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=6,
        restore_best_weights=True,
        verbose=1
    ),
    callbacks.CSVLogger(LOG_FILE, append=True)
]

print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è Stage 2 ({EPOCHS_STAGE2} —ç–ø–æ—Ö —Å Mixup + CosineRestarts)...")
print()

# Validation –¥–ª—è Stage 2: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ one-hot (–¥–ª—è CategoricalCrossentropy)
ds_val_stage2 = ds_val_prep.map(lambda img, lbl: (img, tf.one_hot(lbl, 101)))

history2 = model.fit(
    ds_train_stage2,
    validation_data=ds_val_stage2,
    epochs=EPOCHS_STAGE2,
    callbacks=cbs_stage2,
    verbose=1,
    initial_epoch=EPOCHS_STAGE1
)

# =============================================================================
# –†–ï–ó–£–õ–¨–¢–ê–¢–´
# =============================================================================

print()
print("=" * 80)
print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
print("=" * 80)
print()

all_val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']
all_val_top5 = history1.history['val_top5'] + history2.history['val_top5']

best_acc = max(all_val_acc)
best_top5 = max(all_val_top5)

print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å:     {initial_scores[1]*100:.2f}%")
print(f"   –ü–æ—Å–ª–µ Stage 1:       {stage1_best*100:.2f}%")
print(f"   –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å:    {best_acc*100:.2f}%")
print(f"   Top-5 Accuracy:      {best_top5*100:.2f}%")
print()

improvement = (best_acc * 100) - (initial_scores[1] * 100)
print(f"üìà –ü–†–ò–†–û–°–¢: {improvement:+.2f}%")
print()

if best_acc >= 0.79:
    print("üéâüéâüéâ –ü–†–ï–í–û–°–•–û–î–ù–û! –î–æ—Å—Ç–∏–≥–Ω—É—Ç 79%+")
elif best_acc >= 0.76:
    print("üéâ –û–¢–õ–ò–ß–ù–û! –î–æ—Å—Ç–∏–≥–Ω—É—Ç 76%+")
elif best_acc >= 0.73:
    print("‚úÖ –•–æ—Ä–æ—à–æ! –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ")
elif improvement > 0:
    print("‚úÖ –ï—Å—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å")
else:
    print("‚ö†Ô∏è  –ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

print()
print(f"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {OUTPUT_MODEL_PATH}")
print(f"üìã –õ–æ–≥: {LOG_FILE}")
print()

# =============================================================================
# –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê (–° EMA –í–ï–°–ê–ú–ò!)
# =============================================================================

print("üî¨ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ–º validation set...")
final_model = keras.models.load_model(OUTPUT_MODEL_PATH)

# –î–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏: sparse
final_model.compile(
    optimizer='adam',
    loss=keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy', keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5')]
)

final_scores = final_model.evaluate(ds_val_prep, verbose=1)
print()
print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (—Å EMA –≤–µ—Å–∞–º–∏):")
print(f"   Accuracy: {final_scores[1]*100:.2f}%")
print(f"   Top-5: {final_scores[2]*100:.2f}%")
print()

print("=" * 80)
print("üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
if final_scores[1] >= 0.76:
    print("   üéâüéâüéâ –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É!")
    print()
    print("   –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ API:")
    print("   1. pkill -f api_server.py")
    print("   2. –í api_server.py:")
    print("      model = keras.models.load_model('models/texture_improved_perfect.keras')")
    print("   3. python3 api_server.py")
elif final_scores[1] >= 0.73:
    print("   ‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    print("   –ú–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å –µ—â—ë 5-10 —ç–ø–æ—Ö")
else:
    print("   ‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ")
    print("   –£–≤–µ–ª–∏—á—å—Ç–µ EPOCHS_STAGE2 –¥–æ 30")

print("=" * 80)
