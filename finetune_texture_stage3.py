#!/usr/bin/env python3
"""
STAGE 3: Full Fine-Tuning –¥–ª—è texture_improved_perfect.keras
–¶–µ–ª—å: 75.44% ‚Üí 82-84%

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
‚úÖ –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –í–°–ï–ô –º–æ–¥–µ–ª–∏ (–≤–∫–ª—é—á–∞—è conv1, conv2, conv3)
‚úÖ Batch size = 64 (–±—ã–ª–æ 16)
‚úÖ Warmup (3 —ç–ø–æ—Ö–∏) + Cosine Decay
‚úÖ AdamW optimizer —Å weight decay
‚úÖ –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
‚úÖ Label smoothing = 0.05
‚úÖ Early stopping patience = 10
‚úÖ –ë–ï–ó Mixup (—á–∏—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
import keras
from keras import layers, optimizers, callbacks
import tensorflow_datasets as tfds
import numpy as np
import gc
from datetime import datetime

print("=" * 80)
print("üöÄ STAGE 3: FULL FINE-TUNING TEXTURE-AWARE MODEL")
print("=" * 80)
print()

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =============================================================================

INPUT_MODEL = 'models/texture_improved_perfect.keras'
OUTPUT_MODEL = 'models/texture_improved_perfect_v2.keras'
LOG_FILE = 'training_stage3.log'

# Batch –∏ –¥–∞–Ω–Ω—ã–µ
BATCH_SIZE = 64  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 16GB RAM + RTX 2070 SUPER
BUFFER_SIZE = 20000  # –ü–æ–ª–Ω—ã–π shuffle buffer

# –û–±—É—á–µ–Ω–∏–µ
EPOCHS = 30  # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
WARMUP_EPOCHS = 3  # –ü–æ–ª–Ω—ã–π warmup

# Learning rates
WARMUP_LR = 1e-7
INITIAL_LR = 1e-5
MIN_LR = 1e-8

# –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
LABEL_SMOOTHING = 0.05
WEIGHT_DECAY = 1e-5

print("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
print(f"   Batch size:          {BATCH_SIZE}")
print(f"   Buffer size:         {BUFFER_SIZE}")
print(f"   Epochs:              {EPOCHS}")
print(f"   Warmup epochs:       {WARMUP_EPOCHS}")
print(f"   Initial LR:          {INITIAL_LR}")
print(f"   Min LR:              {MIN_LR}")
print(f"   Label smoothing:     {LABEL_SMOOTHING}")
print(f"   Weight decay:        {WEIGHT_DECAY}")
print()

# =============================================================================
# MIXED PRECISION
# =============================================================================

print("üöÄ –í–∫–ª—é—á–µ–Ω–∏–µ Mixed Precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ RTX 2070 SUPER...")
# Mixed precision –¥–ª—è RTX GPU - —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ ~2 —Ä–∞–∑–∞
keras.mixed_precision.set_global_policy('mixed_float16')
print("‚úÖ Mixed Precision (FP16) –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
print()

# =============================================================================
# –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò
# =============================================================================

print("üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
keras.backend.clear_session()
gc.collect()
print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
print()

# =============================================================================
# WARMUP + COSINE DECAY LR SCHEDULE
# =============================================================================

class WarmupCosineDecay(keras.optimizers.schedules.LearningRateSchedule):
    """
    Warmup (3 —ç–ø–æ—Ö–∏): 1e-7 ‚Üí 1e-5
    CosineDecay (27 —ç–ø–æ—Ö): 1e-5 ‚Üí 1e-8
    """

    def __init__(self, warmup_steps, total_steps,
                 warmup_lr=1e-7, initial_lr=1e-5, min_lr=1e-8):
        super().__init__()
        self.warmup_steps = tf.cast(warmup_steps, tf.float32)
        self.total_steps = tf.cast(total_steps, tf.float32)
        self.warmup_lr = warmup_lr
        self.initial_lr = initial_lr
        self.min_lr = min_lr

    def __call__(self, step):
        step = tf.cast(step, tf.float32)

        # Warmup phase
        warmup_progress = step / self.warmup_steps
        warmup_lr = self.warmup_lr + (self.initial_lr - self.warmup_lr) * warmup_progress

        # Cosine decay phase
        decay_steps = self.total_steps - self.warmup_steps
        step_in_decay = step - self.warmup_steps
        cosine_decay = 0.5 * (1.0 + tf.cos(
            tf.constant(np.pi) * step_in_decay / decay_steps
        ))
        decay_lr = self.min_lr + (self.initial_lr - self.min_lr) * cosine_decay

        # –í—ã–±–∏—Ä–∞–µ–º –º–µ–∂–¥—É warmup –∏ decay
        return tf.cond(
            step < self.warmup_steps,
            lambda: warmup_lr,
            lambda: decay_lr
        )

    def get_config(self):
        return {
            'warmup_steps': float(self.warmup_steps.numpy()),
            'total_steps': float(self.total_steps.numpy()),
            'warmup_lr': self.warmup_lr,
            'initial_lr': self.initial_lr,
            'min_lr': self.min_lr
        }

# =============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# =============================================================================

print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model = keras.models.load_model(INPUT_MODEL)
print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
print()

# =============================================================================
# –†–ê–ó–ú–û–†–û–ó–ö–ê –í–°–ï–ô –ú–û–î–ï–õ–ò
# =============================================================================

print("üîì –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –í–°–ï–ô –º–æ–¥–µ–ª–∏...")

def unfreeze_all_layers(model):
    """–†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –í–°–ï —Å–ª–æ–∏, –≤–∫–ª—é—á–∞—è nested ResNet50"""
    total_unfrozen = 0

    for layer in model.layers:
        # Nested model (ResNet50)
        if hasattr(layer, 'layers'):
            for sublayer in layer.layers:
                if not sublayer.trainable:
                    sublayer.trainable = True
                    total_unfrozen += 1
        else:
            if not layer.trainable:
                layer.trainable = True
                total_unfrozen += 1

    return total_unfrozen

unfrozen = unfreeze_all_layers(model)
print(f"‚úÖ –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω–æ {unfrozen} —Å–ª–æ—ë–≤")

trainable = sum([tf.size(w).numpy() for w in model.trainable_weights])
total = model.count_params()
print(f"   Trainable params: {trainable:,} ({trainable/total*100:.1f}%)")
print()

# =============================================================================
# –î–ê–¢–ê–°–ï–¢
# =============================================================================

print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ Food-101...")
ds_train, ds_val = tfds.load(
    'food101',
    split=['train', 'validation'],
    as_supervised=True,
    shuffle_files=True
)
print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω (train: 75750, val: 25250)")
print()

# =============================================================================
# –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–ê–ì–†–ï–°–°–ò–í–ù–ê–Ø!)
# =============================================================================

def resize_only(image, label):
    """Resize –¥–æ 224x224"""
    image = tf.image.resize(image, [224, 224])
    return image, label

def augment_aggressive(image, label):
    """
    –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è Stage 3
    –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ full fine-tuning
    """
    # Geometric
    image = tf.image.random_flip_left_right(image)

    # Random zoom —á–µ—Ä–µ–∑ crop_to_bounding_box + resize
    # Zoom 0.7-1.0 –æ–∑–Ω–∞—á–∞–µ—Ç crop –æ—Ç 70% –¥–æ 100% –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    zoom_factor = tf.random.uniform([], 0.7, 1.0)
    crop_size = tf.cast(224.0 * zoom_factor, tf.int32)

    # –°–ª—É—á–∞–π–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è crop
    offset_h = tf.random.uniform([], 0, 224 - crop_size + 1, dtype=tf.int32)
    offset_w = tf.random.uniform([], 0, 224 - crop_size + 1, dtype=tf.int32)

    image = tf.image.crop_to_bounding_box(image, offset_h, offset_w, crop_size, crop_size)
    image = tf.image.resize(image, [224, 224])

    # Color jitter (—É—Å–∏–ª–µ–Ω–Ω–æ–µ)
    image = tf.image.random_brightness(image, 0.3)
    image = tf.image.random_contrast(image, 0.7, 1.4)
    image = tf.image.random_saturation(image, 0.7, 1.4)
    image = tf.image.random_hue(image, 0.08)

    # Gaussian noise (–¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏)
    if tf.random.uniform([]) > 0.5:
        noise = tf.random.normal(tf.shape(image), mean=0.0, stddev=0.05)
        image = image + noise

    return image, label

def preprocess_for_model(image, label):
    """ResNet50 preprocessing"""
    image = keras.applications.resnet50.preprocess_input(image)
    return image, label

# =============================================================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–û–í
# =============================================================================

print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")

def convert_to_onehot(image, label):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è sparse label –≤ one-hot –¥–ª—è CategoricalCrossentropy"""
    return image, tf.one_hot(label, 101)

# Training: resize ‚Üí augment ‚Üí preprocess ‚Üí batch ‚Üí one-hot
ds_train_prep = (
    ds_train
    .shuffle(BUFFER_SIZE)
    .map(resize_only, num_parallel_calls=tf.data.AUTOTUNE)
    .map(augment_aggressive, num_parallel_calls=tf.data.AUTOTUNE)
    .map(preprocess_for_model, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .map(lambda img, lbl: (img, tf.one_hot(lbl, 101)), num_parallel_calls=tf.data.AUTOTUNE)
    .prefetch(tf.data.AUTOTUNE)
)

# Validation: resize ‚Üí preprocess ‚Üí batch ‚Üí one-hot
ds_val_prep = (
    ds_val
    .map(resize_only, num_parallel_calls=tf.data.AUTOTUNE)
    .map(preprocess_for_model, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .map(lambda img, lbl: (img, tf.one_hot(lbl, 101)), num_parallel_calls=tf.data.AUTOTUNE)
    .prefetch(tf.data.AUTOTUNE)
)

print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã –≥–æ—Ç–æ–≤—ã (—Å one-hot labels)")
print()

# =============================================================================
# LR SCHEDULE
# =============================================================================

steps_per_epoch = 75750 // BATCH_SIZE  # ~1183 steps (batch=64)
warmup_steps = steps_per_epoch * WARMUP_EPOCHS
total_steps = steps_per_epoch * EPOCHS

lr_schedule = WarmupCosineDecay(
    warmup_steps=warmup_steps,
    total_steps=total_steps,
    warmup_lr=WARMUP_LR,
    initial_lr=INITIAL_LR,
    min_lr=MIN_LR
)

print(f"üìà LR Schedule:")
print(f"   Steps per epoch:     {steps_per_epoch}")
print(f"   Warmup steps:        {warmup_steps}")
print(f"   Total steps:         {total_steps}")
print(f"   Warmup: {WARMUP_LR} ‚Üí {INITIAL_LR}")
print(f"   Decay:  {INITIAL_LR} ‚Üí {MIN_LR}")
print()

# =============================================================================
# OPTIMIZER & COMPILATION
# =============================================================================

print("‚öôÔ∏è  –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏...")

# AdamW —Å weight decay
optimizer = optimizers.Adam(
    learning_rate=lr_schedule,
    clipnorm=1.0,  # Gradient clipping
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-7
)

# Loss —Å label smoothing
# –î–ª—è Keras 3.x –∏—Å–ø–æ–ª—å–∑—É–µ–º CategoricalCrossentropy —Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π labels
loss = keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=LABEL_SMOOTHING
)

# Metrics (–¥–ª—è CategoricalCrossentropy)
metrics = [
    'accuracy',
    keras.metrics.TopKCategoricalAccuracy(k=5, name='top5')
]

model.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=metrics
)

print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞")
print(f"   Optimizer: Adam")
print(f"   Loss: SparseCategoricalCrossentropy (label_smoothing={LABEL_SMOOTHING})")
print(f"   Metrics: accuracy, top5")
print()

# =============================================================================
# –ü–†–û–í–ï–†–ö–ê –ù–ê–ß–ê–õ–¨–ù–û–ô –¢–û–ß–ù–û–°–¢–ò
# =============================================================================

print("üî¨ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏...")
initial_scores = model.evaluate(
    ds_val_prep.take(100),
    verbose=0
)
print(f"üìä –ù–∞—á–∞–ª—å–Ω–∞—è accuracy: {initial_scores[1]*100:.2f}%")
print(f"   Top-5 accuracy:     {initial_scores[2]*100:.2f}%")
print()

# =============================================================================
# CALLBACKS
# =============================================================================

cbs = [
    # 1. Model Checkpoint
    callbacks.ModelCheckpoint(
        OUTPUT_MODEL,
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),

    # 2. Early Stopping (patience=10)
    callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=10,  # –ü–æ–ª–Ω—ã–π patience –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        restore_best_weights=True,
        mode='max',
        verbose=1
    ),

    # 3. CSV Logger
    callbacks.CSVLogger(
        LOG_FILE,
        append=False
    ),

    # 4. ReduceLROnPlateau (backup)
    callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-9,
        verbose=1
    ),

    # 5. Progress callback
    callbacks.LambdaCallback(
        on_epoch_end=lambda epoch, logs: print(
            f"\nüìä Epoch {epoch+1}/{EPOCHS}: "
            f"val_acc={logs.get('val_accuracy', 0)*100:.2f}%, "
            f"val_top5={logs.get('val_top5', 0)*100:.2f}%"
        )
    )
]

# =============================================================================
# –û–ë–£–ß–ï–ù–ò–ï
# =============================================================================

print("=" * 80)
print("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø STAGE 3")
print("=" * 80)
print()
print(f"‚è∞ –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~45-60 –º–∏–Ω—É—Ç ({EPOCHS} —ç–ø–æ—Ö, batch={BATCH_SIZE})")
print(f"‚ö° –° early stopping –≤–æ–∑–º–æ–∂–Ω–æ ~30-40 –º–∏–Ω—É—Ç")
print(f"‚öôÔ∏è  –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è 16GB RAM + RTX 2070 SUPER")
print()

start_time = datetime.now()

history = model.fit(
    ds_train_prep,
    validation_data=ds_val_prep,
    epochs=EPOCHS,
    callbacks=cbs,
    verbose=1
)

end_time = datetime.now()
duration = (end_time - start_time).total_seconds() / 60

# =============================================================================
# –†–ï–ó–£–õ–¨–¢–ê–¢–´
# =============================================================================

print()
print("=" * 80)
print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
print("=" * 80)
print()

best_acc = max(history.history['val_accuracy'])
best_top5 = max(history.history['val_top5'])
best_epoch = history.history['val_accuracy'].index(best_acc) + 1

print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {duration:.1f} –º–∏–Ω—É—Ç")
print()
print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"   –ù–∞—á–∞–ª—å–Ω–∞—è accuracy:  {initial_scores[1]*100:.2f}%")
print(f"   –õ—É—á—à–∞—è accuracy:     {best_acc*100:.2f}% (—ç–ø–æ—Ö–∞ {best_epoch})")
print(f"   –õ—É—á—à–∞—è top-5:        {best_top5*100:.2f}%")
print()

improvement = (best_acc - initial_scores[1]) * 100
print(f"üìà –ü–†–ò–†–û–°–¢: {improvement:+.2f}%")
print()

# –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
if best_acc >= 0.84:
    print("üéâüéâüéâ –ü–†–ï–í–û–°–•–û–î–ù–û! –î–æ—Å—Ç–∏–≥–Ω—É—Ç 84%+")
    print("   –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É!")
elif best_acc >= 0.82:
    print("üéâüéâ –û–¢–õ–ò–ß–ù–û! –î–æ—Å—Ç–∏–≥–Ω—É—Ç 82%+")
    print("   –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–∑–æ—à–ª–∞ –±–∞–∑–æ–≤—É—é ResNet50!")
elif best_acc >= 0.81:
    print("üéâ –•–û–†–û–®–û! –î–æ—Å—Ç–∏–≥–Ω—É—Ç 81%+")
    print("   –ú–æ–¥–µ–ª—å —Å—Ä–∞–≤–Ω—è–ª–∞—Å—å —Å –±–∞–∑–æ–≤–æ–π!")
elif best_acc >= 0.78:
    print("‚úÖ –•–æ—Ä–æ—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ!")
    print("   –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ—â—ë 10-20 —ç–ø–æ—Ö")
else:
    print("‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∏–∂–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ")
    print(f"   –î–æ —Ü–µ–ª–µ–≤—ã—Ö 82%: {(0.82 - best_acc)*100:.2f}%")

print()
print(f"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {OUTPUT_MODEL}")
print(f"üìã –õ–æ–≥ –æ–±—É—á–µ–Ω–∏—è:  {LOG_FILE}")
print()

# =============================================================================
# –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –ü–û–õ–ù–û–ú VAL SET
# =============================================================================

print("üî¨ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ–º validation set...")
final_model = keras.models.load_model(OUTPUT_MODEL)

final_model.compile(
    optimizer='adam',
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top5')]
)

final_scores = final_model.evaluate(ds_val_prep, verbose=1)
print()
print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:")
print(f"   Accuracy: {final_scores[1]*100:.2f}%")
print(f"   Top-5:    {final_scores[2]*100:.2f}%")
print()

# =============================================================================
# –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
# =============================================================================

print("=" * 80)
print("üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
print("=" * 80)
print()

if final_scores[1] >= 0.81:
    print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    print()
    print("–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ API:")
    print("1. pkill -f api_server_with_tta.py")
    print("2. sed -i '' 's/texture_improved_perfect.keras/texture_improved_perfect_v2.keras/g' api_server_with_tta.py")
    print(f"3. sed -i '' 's/75.44%/{final_scores[1]*100:.2f}%/g' api_server_with_tta.py")
    print("4. python3 api_server_with_tta.py &")
elif final_scores[1] >= 0.78:
    print("‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ú–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å –µ—â—ë:")
    print()
    print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ –µ—â—ë 10-20 —ç–ø–æ—Ö:")
    print("1. –ò–∑–º–µ–Ω–∏—Ç–µ EPOCHS –Ω–∞ 10-20")
    print("2. –ò–∑–º–µ–Ω–∏—Ç–µ INPUT_MODEL –Ω–∞ texture_improved_perfect_v2.keras")
    print("3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞")
else:
    print("‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞:")
    print()
    print("–í–∞—Ä–∏–∞–Ω—Ç—ã:")
    print("1. –£–≤–µ–ª–∏—á–∏—Ç—å EPOCHS –¥–æ 40-50")
    print("2. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å BATCH_SIZE = 32 (–µ—Å–ª–∏ 64 —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ)")
    print("3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å class weights –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤")

print()
print("=" * 80)
print("‚ú® STAGE 3 –ó–ê–í–ï–†–®–Å–ù")
print("=" * 80)
